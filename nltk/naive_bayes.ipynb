{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Spam accuracy: 0.9907\n",
      "Test Ham accuracy: 0.9250\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.probability import *\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from collections import defaultdict\n",
    "#------------------------------------get files from directory------------------------------------------------------------\n",
    "def getFilesFromDir (path):\n",
    "    dir_content = os.listdir(path)\n",
    "    dir_clean = filter(lambda x: (\".DS_Store\" not in x) and (\"cmds\" not in x), dir_content)\n",
    "    msg = map(lambda x: getMessage(path + '/' + x), dir_clean)\n",
    "    return msg\n",
    "#------------------------------------get message from files--------------------------------------------------------------\n",
    "def getMessage (path):\n",
    "    fo = open (path)\n",
    "    lines = fo.readlines()\n",
    "    fo.close()\n",
    "#get message body\n",
    "    lines = ''.join(lines)\n",
    "    body_position_index = lines.index('\\n\\n')\n",
    "    lines = lines[body_position_index + 1: len(lines)]\n",
    "    return lines\n",
    "#--------------------------parameter setting-----------------------------------------------------------------------------\n",
    "#set amount of samples to take, 500 samples equals to the number of spam samples\n",
    "amountOfSamplesPerSet = 500\n",
    "amountOfFeaturesPerSet = 400#features can be 50, 100, 200, 400...\n",
    "#file path\n",
    "base_path = \"/Users/yuliang/Downloads/naive_bayes\"\n",
    "ham_path = base_path + \"/easy_ham\"\n",
    "ham_path2 = base_path + \"/easy_ham_2\"\n",
    "spam_path = base_path + \"/spam\"\n",
    "spam_path2 = base_path + \"/spam_2\"\n",
    "#get files from directory\n",
    "hamTrainDir = getFilesFromDir(ham_path)\n",
    "hamTrainDir = hamTrainDir[:amountOfSamplesPerSet]\n",
    "hamTestDir = getFilesFromDir(ham_path2)\n",
    "spamTrainDir = getFilesFromDir(spam_path)\n",
    "spamTrainDir = spamTrainDir[:amountOfSamplesPerSet]\n",
    "spamTestDir = getFilesFromDir(spam_path2)\n",
    "#-------------------------get message words from data--------------------------------------------------------------------\n",
    "def getMessageWords(file_msg, stopwords = []):\n",
    "    file_msg = ''.join(file_msg)\n",
    "    file_msg = re.sub('3D', '', file_msg)\n",
    "    file_msg = re.sub(r'([^\\s\\w]|_)+', '', file_msg)\n",
    "    \n",
    "    file_msg_words = wordpunct_tokenize(file_msg.replace('=\\n', '').lower())\n",
    "    file_msg_words = filter(lambda x: x not in stopwords, file_msg_words)\n",
    "    file_msg_words = [w for w in file_msg_words if re.search('[a-zA-Z]', w) and len(w) > 1]\n",
    "    return file_msg_words\n",
    "\n",
    "def getMessageWords2(file_msg, stopwords = []):\n",
    "    file_msg = re.sub('3D', '', file_msg)\n",
    "    file_msg = re.sub(r'([^\\s\\w]|_)+', '', file_msg)\n",
    "    \n",
    "    file_msg_words = wordpunct_tokenize(file_msg.replace('=\\n', '').lower())\n",
    "    file_msg_words = filter(lambda x: x not in stopwords, file_msg_words)\n",
    "    file_msg_words = [w for w in file_msg_words if re.search('[a-zA-Z]', w) and len(w) > 1]\n",
    "    return file_msg_words\n",
    "#-------------------------get stop words---------------------------------------------------------------------------------\n",
    "def getStopWords (path):\n",
    "    fo = open (path)\n",
    "    lines = fo.readlines()\n",
    "    lines_clean = map(lambda x: str.replace(x, '\\n', ''), lines)\n",
    "    fo.close()\n",
    "    return lines_clean\n",
    "stop_words_given = getStopWords (\"/Users/yuliang/Downloads/naive_bayes/stopwords.txt\")\n",
    "#-----------------------------------Term Document Matrix-----------------------------------------------------------------\n",
    "#get features\n",
    "def getFeatures(file_msg, **kwargs):\n",
    "    file_msg_words = getMessageWords(file_msg, **kwargs)\n",
    "    words_list = nltk.FreqDist(file_msg_words)\n",
    "    words_list_common = words_list.most_common()\n",
    "    topFeatures = map(lambda x: x[0], words_list_common[:amountOfFeaturesPerSet])\n",
    "    return topFeatures\n",
    "\n",
    "#filter stop words\n",
    "hamFeatures = getFeatures(hamTrainDir, stopwords = stop_words_given)\n",
    "spamFeatures = getFeatures(spamTrainDir, stopwords = stop_words_given)\n",
    "\n",
    "#combine features\n",
    "allFeatures = set(hamFeatures + spamFeatures)\n",
    "allFeatures = list(allFeatures)\n",
    "\n",
    "#feature label\n",
    "def getFeaturesLabel(file_msg, label, allFeature, feature_extractor, **kwargs):\n",
    "    features = map(lambda x: feature_extractor(x, allFeature, **kwargs), file_msg)\n",
    "    features_label = map(lambda x: (x, label), features)\n",
    "    return features_label\n",
    "\n",
    "#words indicator\n",
    "def wordsIndicator(file_msg, allFeature, **kwargs):\n",
    "    file_msg_words = getMessageWords2(file_msg, **kwargs)\n",
    "    featureWords = allFeature\n",
    "    features_dict = defaultdict(list) \n",
    "    for w in file_msg_words:\n",
    "        if w in allFeature:\n",
    "            features_dict[w] = True\n",
    "    return features_dict\n",
    "#------------------------------------Train and test dataset--------------------------------------------------------------\n",
    "hamTrainFeature = getFeaturesLabel(hamTrainDir, 'ham', allFeatures, wordsIndicator, stopwords = stop_words_given)\n",
    "spamTrainFeature = getFeaturesLabel(spamTrainDir, 'spam', allFeatures, wordsIndicator, stopwords = stop_words_given)\n",
    "trainFeature = hamTrainFeature + spamTrainFeature\n",
    "hamTestFeature = getFeaturesLabel(hamTestDir, 'ham', allFeatures, wordsIndicator, stopwords = stop_words_given)\n",
    "spamTestFeature = getFeaturesLabel(spamTestDir, 'spam', allFeatures, wordsIndicator, stopwords = stop_words_given)\n",
    "#------------------------------------Naive Bayes Classifier--------------------------------------------------------------\n",
    "# Train the naive bayes classifier\n",
    "naive_bayes_classifier = NaiveBayesClassifier.train(trainFeature)\n",
    "print ('Test Spam accuracy: %.4f' %nltk.classify.accuracy(naive_bayes_classifier, spamTestFeature))\n",
    "print ('Test Ham accuracy: %.4f' %nltk.classify.accuracy(naive_bayes_classifier, hamTestFeature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

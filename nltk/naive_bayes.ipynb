{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Spam accuracy: 0.8507\n",
      "Test Ham accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.probability import *\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from collections import defaultdict\n",
    "#------------------------------------load directory and clean useless files--------------------------------------------------------------------\n",
    "def getFilesFromDir (path):\n",
    "    dir_content = os.listdir(path)\n",
    "    dir_clean = filter(lambda x: (\".DS_Store\" not in x) and (\"cmds\" not in x), dir_content)\n",
    "    \n",
    "    msg = [\"\" for x in range(len(dir_clean))]\n",
    "    for index in range (len(dir_clean)):\n",
    "        file_path = path + '/' + dir_clean[index]\n",
    "        msg[index] = getMessage(file_path)\n",
    "    return msg\n",
    "#------------------------------------load data in each file in the directory--------------------\n",
    "def getMessage (path):\n",
    "    fo = open (path)\n",
    "    lines = fo.readlines()\n",
    "    fo.close()\n",
    "        \n",
    "    for index in range (len(lines)):\n",
    "        if (index <= (len(lines)-2)):\n",
    "            if (('\\n' in lines[index]) and (lines[index + 1] == '\\n')):\n",
    "                lines[index:index+2] = [''.join(lines[index:index+2])]\n",
    "    #move to the first line break of the email which indicates the message body\n",
    "    start_position = next(x for x in lines if '\\n\\n' in x)\n",
    "    start_position_index = lines.index(start_position)\n",
    "    lines_clean = lines[start_position_index + 1: len(lines)]\n",
    "    #convert list into string\n",
    "    str_msg = ''.join(lines_clean)\n",
    "    return str_msg\n",
    "\n",
    "amountOfSamplesPerSet = 500\n",
    "amountOfFeaturesPerSet = 100\n",
    "#file path\n",
    "base_path = \"/Users/yuliang/Downloads/naive_bayes\"\n",
    "ham_path = base_path + \"/easy_ham\"\n",
    "ham_path2 = base_path + \"/easy_ham_2\"\n",
    "spam_path = base_path + \"/spam\"\n",
    "spam_path2 = base_path + \"/spam_2\"\n",
    "#get files from directory\n",
    "hamTrainDir = getFilesFromDir(ham_path)\n",
    "hamTrainDir = hamTrainDir[:amountOfSamplesPerSet]\n",
    "hamTestDir = getFilesFromDir(ham_path2)\n",
    "spamTrainDir = getFilesFromDir(spam_path)\n",
    "spamTrainDir = spamTrainDir[:amountOfSamplesPerSet]\n",
    "spamTestDir = getFilesFromDir(spam_path2)\n",
    "#-------------------------get message words from data------------------------------------------\n",
    "def getMessageWords(file_msg, stopwords = []):\n",
    "    file_msg = ''.join(file_msg)\n",
    "    file_msg = re.sub('3D', '', file_msg)\n",
    "    file_msg = re.sub(r'([^\\s\\w]|_)+', '', file_msg)\n",
    "    \n",
    "    file_msg_words = wordpunct_tokenize(file_msg.replace('=\\n', '').lower())\n",
    "    file_msg_words = filter(lambda x: x not in stopwords, file_msg_words)\n",
    "    file_msg_words = [w for w in file_msg_words if re.search('[a-zA-Z]', w) and len(w) > 1]\n",
    "    return file_msg_words\n",
    "\n",
    "def getMessageWords2(file_msg, stopwords = []):\n",
    "    file_msg = re.sub('3D', '', file_msg)\n",
    "    file_msg = re.sub(r'([^\\s\\w]|_)+', '', file_msg)\n",
    "    \n",
    "    file_msg_words = wordpunct_tokenize(file_msg.replace('=\\n', '').lower())\n",
    "    file_msg_words = filter(lambda x: x not in stopwords, file_msg_words)\n",
    "    file_msg_words = [w for w in file_msg_words if re.search('[a-zA-Z]', w) and len(w) > 1]\n",
    "    return file_msg_words\n",
    "#-------------------------get stop words--------------------------------------------------------\n",
    "def getStopWords (path):\n",
    "    fo = open (path)\n",
    "    lines = fo.readlines()\n",
    "    lines_clean = map(lambda x: str.replace(x, '\\n', ''), lines)\n",
    "    fo.close()\n",
    "    return lines_clean\n",
    "stop_words_given = getStopWords (\"/Users/yuliang/Downloads/naive_bayes/stopwords.txt\")\n",
    "#-----------------------------------Term Document Matrix---------------------------------------------------------------\n",
    "#get features\n",
    "def getFeatures(file_msg, **kwargs):\n",
    "    file_msg_words = getMessageWords(file_msg, **kwargs)\n",
    "    words_list = nltk.FreqDist(file_msg_words)\n",
    "    words_list_common = words_list.most_common()\n",
    "    topFeatures = [\"\" for x in range(amountOfFeaturesPerSet)]\n",
    "    for index in range(amountOfFeaturesPerSet):\n",
    "        topFeatures[index]= words_list_common[index][0]\n",
    "    return topFeatures\n",
    "\n",
    "hamFeatures = getFeatures(hamTrainDir, stopwords = stop_words_given)\n",
    "spamFeatures = getFeatures(spamTrainDir, stopwords = stop_words_given)\n",
    "allFeatures = set(hamFeatures + spamFeatures)\n",
    "allFeatures = list(allFeatures)\n",
    "#print(trainFeatures)\n",
    "\n",
    "def getFeaturesLabel(file_msg, label, allFeature, feature_extractor, **kwargs):\n",
    "    features_label = []\n",
    "    for w in file_msg:\n",
    "        features = feature_extractor(w, allFeature, **kwargs)\n",
    "        features_label.append((features, label))\n",
    "    return features_label\n",
    "\n",
    "def wordsIndicator(file_msg, allFeature, **kwargs):\n",
    "    file_msg_words = getMessageWords2(file_msg, **kwargs)\n",
    "    featureWords = allFeature\n",
    "    features_dict = defaultdict(list)\n",
    "    \n",
    "    for w in file_msg_words:\n",
    "        if w in allFeature:\n",
    "            features_dict[w] = True\n",
    "    return features_dict\n",
    "#------------------------------------Train and test dataset---------------------------------------------\n",
    "hamTrainFeature = getFeaturesLabel(hamTrainDir, 'ham', allFeatures, wordsIndicator, stopwords = stop_words_given)\n",
    "spamTrainFeature = getFeaturesLabel(spamTrainDir, 'spam', allFeatures, wordsIndicator, stopwords = stop_words_given)\n",
    "trainFeature = hamTrainFeature + spamTrainFeature\n",
    "hamTestFeature = getFeaturesLabel(hamTestDir, 'ham', allFeatures, wordsIndicator, stopwords = stop_words_given)\n",
    "spamTestFeature = getFeaturesLabel(spamTestDir, 'spam', allFeatures, wordsIndicator, stopwords = stop_words_given)\n",
    "#------------------------------------Naive Bayes Classifier----------------------------------------------  \n",
    "# Train the naive bayes classifier\n",
    "naive_bayes_classifier = NaiveBayesClassifier.train(trainFeature)\n",
    "print ('Test Spam accuracy: %.4f' %nltk.classify.accuracy(naive_bayes_classifier, hamTestFeature))\n",
    "print ('Test Ham accuracy: %.4f' %nltk.classify.accuracy(naive_bayes_classifier, spamTestFeature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

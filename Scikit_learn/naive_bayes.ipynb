{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973381294964\n",
      "0.842446043165\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#------------------------------------load directory and clean useless files--------------------------------------------------------------------\n",
    "def getFilesFromDir (path):\n",
    "    dir_content = os.listdir(path)\n",
    "    dir_clean = filter(lambda x: (\".DS_Store\" not in x) and (\"cmds\" not in x), dir_content)\n",
    "    return dir_clean\n",
    "#file path\n",
    "base_path = \"/Users/yuliang/Downloads/naive_bayes\"\n",
    "ham_path = base_path + \"/easy_ham\"\n",
    "ham_path2 = base_path + \"/easy_ham_2\"\n",
    "spam_path = base_path + \"/spam\"\n",
    "spam_path2 = base_path + \"/spam_2\"\n",
    "#get files from directory\n",
    "file_list_ham = getFilesFromDir(ham_path)\n",
    "file_list_ham2 = getFilesFromDir(ham_path2)\n",
    "file_list_spam = getFilesFromDir(spam_path)\n",
    "file_list_spam2 = getFilesFromDir(spam_path2)\n",
    "#------------------------------------load data in each file in the directory---------------------------------------------\n",
    "def getMessage (file_list, path, amount_of_samples):\n",
    "    strip_html = False\n",
    "    str_list = [\"\" for x in range (amount_of_samples)]\n",
    "    i = 0\n",
    "#open files in the directory\n",
    "    for index in range (amount_of_samples):\n",
    "        fo = open (path + '/' + file_list[index])\n",
    "        lines = fo.readlines()\n",
    "        fo.close()\n",
    "\n",
    "        for index in range (len(lines)):\n",
    "            if (index <= (len(lines)-2)):\n",
    "                if (('\\n' in lines[index]) and (lines[index + 1] == '\\n')):\n",
    "                    lines[index:index+2] = [''.join(lines[index:index+2])]\n",
    "#move to the first line break of the email which indicates the message body\n",
    "        start_position = next(x for x in lines if '\\n\\n' in x)\n",
    "        start_position_index = lines.index(start_position)\n",
    "        lines_clean = lines[start_position_index + 1: len(lines)]\n",
    "#convert list into string\n",
    "        str1 = ''.join(lines_clean)\n",
    "#only keep words and spaces in the string\n",
    "        str2 = re.sub(r'([^\\s\\w]|_)+', '', str1)\n",
    "        str_list[i] = str2\n",
    "        i = i + 1     \n",
    "    return str_list\n",
    "#set amount of samples to take, 500 samples equals to the number of spam samples\n",
    "amountOfSamplesPerSet = 500\n",
    "amountOfFeaturesToTake = 50 #features can be 50, 100, 200, 400...\n",
    "amountOfSamplesPerSet_test = 1390 #amount of spam and ham to take for testing\n",
    "\n",
    "str_clean_ham = getMessage (file_list_ham, ham_path, amountOfSamplesPerSet)\n",
    "str_clean_ham2 = getMessage (file_list_ham2, ham_path2, amountOfSamplesPerSet_test)\n",
    "str_clean_spam = getMessage (file_list_spam, spam_path, amountOfSamplesPerSet)\n",
    "str_clean_spam2 = getMessage (file_list_spam2, spam_path2, amountOfSamplesPerSet_test)\n",
    "\n",
    "email_words_list = str_clean_ham + str_clean_spam\n",
    "#------------------------------------get stop words--------------------------------------------------------------------\n",
    "def getStopWords (path):\n",
    "    fo = open (path)\n",
    "    lines = fo.readlines()\n",
    "    lines_clean = map(lambda x: str.replace(x, '\\n', ''), lines)\n",
    "    fo.close()\n",
    "    return lines_clean\n",
    "stop_words_given = getStopWords (\"/Users/yuliang/Downloads/naive_bayes/stopwords.txt\")\n",
    "#-----------------------------------Term Document Matrix---------------------------------------------------------------\n",
    "#establish TDM using sklearn.feature_extraction.text.CountVectorizer\n",
    "#get ham top features\n",
    "vector = CountVectorizer(stop_words = stop_words_given,\n",
    "                         analyzer='word',\n",
    "                         decode_error = 'ignore',\n",
    "                         max_features = amountOfFeaturesToTake)\n",
    "words_counts = vector.fit_transform(str_clean_ham)\n",
    "words_counts = words_counts.toarray()\n",
    "feature_ham = vector.get_feature_names()\n",
    "\n",
    "#get spam top features\n",
    "vector2 = CountVectorizer(stop_words = stop_words_given,\n",
    "                          analyzer='word',\n",
    "                          decode_error = 'ignore',\n",
    "                          max_features = amountOfFeaturesToTake)\n",
    "\n",
    "words_counts2 = vector2.fit_transform(str_clean_spam)\n",
    "words_counts2 = words_counts2.toarray()\n",
    "feature_spam = vector2.get_feature_names()\n",
    "\n",
    "#ham top features + spam top features\n",
    "feature_email_raw = feature_ham + feature_spam\n",
    "feature_email_clean = set(feature_email_raw)\n",
    "feature_email = list(feature_email_clean)\n",
    "\n",
    "#final TDM\n",
    "vector3 = CountVectorizer(decode_error = 'ignore',\n",
    "                          vocabulary = feature_email)\n",
    "\n",
    "words_counts3 = vector3.fit_transform(email_words_list)\n",
    "words_counts3 = words_counts3.toarray()\n",
    "\n",
    "#---------------------------------------classification: training and testing-------------------------------------------\n",
    "#establish testing labels\n",
    "y1 = np.zeros(amountOfSamplesPerSet_test)\n",
    "y2 = np.ones(amountOfSamplesPerSet_test)\n",
    "#establish training labels\n",
    "y = np.zeros(2*amountOfSamplesPerSet)\n",
    "for index in range(amountOfSamplesPerSet, len(y)):\n",
    "    y[index] = 1\n",
    "#training: Multinomial Naive Bayes\n",
    "clf = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True).fit(words_counts3, y)\n",
    "#ham test\n",
    "new_counts = vector3.transform(str_clean_ham2)\n",
    "#predication\n",
    "predicted = clf.predict(new_counts)\n",
    "#print(predicted)\n",
    "#accuracy score\n",
    "accu_score = clf.score(new_counts, y1)\n",
    "print (accu_score)\n",
    "\n",
    "#spam test\n",
    "new_counts = vector3.transform(str_clean_spam2)\n",
    "#predication\n",
    "predicted = clf.predict(new_counts)\n",
    "#print(predicted)\n",
    "#accuracy score\n",
    "accu_score = clf.score(new_counts, y2)\n",
    "print (accu_score)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Run the code several times and change amount of features to take, we get the following results:\n",
    "###Ham Result | Amount of Features to take | Ham (Correct) | Spam | | ----------------------------------|:-----------------------:| -----:| | 50 | 95.90% | 4.10% | | 100 | 97.34% | 2.66% | | 200 | 98.06% | 1.94% | | 400 | 98.49% | 1.51% |\n",
    "###Spam Result | Amount of Features to take | Ham (Correct) | Spam | | ----------------------------------|:-----------------------:| -----:| | 50 | 75.32% | 24.68%| | 100 | 84.24% | 15.76%| | 200 | 86.04% | 13.96%| | 400 | 90.29% | 9.71% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

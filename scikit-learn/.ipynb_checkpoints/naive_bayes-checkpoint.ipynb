{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.981428571429\n",
      "0.932712956335\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#------------------------------------get files from directory------------------------------------------------------------\n",
    "def getFilesFromDir (path):\n",
    "    dir_content = os.listdir(path)\n",
    "    dir_clean = filter(lambda x: (\".DS_Store\" not in x) and (\"cmds\" not in x), dir_content)\n",
    "    msg = map(lambda x: getMessage(path + '/' + x), dir_clean)\n",
    "    return msg\n",
    "#------------------------------------get message from files--------------------------------------------------------------\n",
    "def getMessage (path):\n",
    "    fo = open (path)\n",
    "    lines = fo.readlines()\n",
    "    fo.close()\n",
    "#get message body\n",
    "    lines = ''.join(lines)\n",
    "    body_position_index = lines.index('\\n\\n')\n",
    "    lines = lines[body_position_index + 1: len(lines)]\n",
    "#regular expression process\n",
    "    lines = re.sub('3D', '', lines)\n",
    "    lines = re.sub(r'([^\\s\\w]|_)+', '', lines)\n",
    "    return lines\n",
    "#------------------------------------parameter setting-------------------------------------------------------------------\n",
    "#set amount of samples to take, 500 samples equals to the number of spam samples\n",
    "amountOfSamplesPerSet = 500\n",
    "amountOfFeaturesToTake = 400 #features can be 50, 100, 200, 400...\n",
    "#file path\n",
    "base_path = \"/Users/yuliang/Downloads/naive_bayes\"\n",
    "ham_path = base_path + \"/easy_ham\"\n",
    "ham_path2 = base_path + \"/easy_ham_2\"\n",
    "spam_path = base_path + \"/spam\"\n",
    "spam_path2 = base_path + \"/spam_2\"\n",
    "#get files from directory\n",
    "hamTrainDir = getFilesFromDir(ham_path)\n",
    "hamTrainDir = hamTrainDir[:amountOfSamplesPerSet]\n",
    "hamTestDir = getFilesFromDir(ham_path2)\n",
    "spamTrainDir = getFilesFromDir(spam_path)\n",
    "spamTrainDir = spamTrainDir[:amountOfSamplesPerSet]\n",
    "spamTestDir = getFilesFromDir(spam_path2)\n",
    "emailTrainDir = hamTrainDir + spamTrainDir\n",
    "#------------------------------------get stop words--------------------------------------------------------------------\n",
    "def getStopWords (path):\n",
    "    fo = open (path)\n",
    "    lines = fo.readlines()\n",
    "    lines = map(lambda x: str.replace(x, '\\n', ''), lines)\n",
    "    fo.close()\n",
    "    return lines\n",
    "stop_words_given = getStopWords (\"/Users/yuliang/Downloads/naive_bayes/stopwords.txt\")\n",
    "#-----------------------------------Term Document Matrix---------------------------------------------------------------\n",
    "#establish Term Document Matrix using sklearn.feature_extraction.text.TfidfVectorizer\n",
    "#get ham top features\n",
    "vector = TfidfVectorizer(stop_words = stop_words_given,\n",
    "                         analyzer='word',\n",
    "                         decode_error = 'ignore',\n",
    "                         max_features = amountOfFeaturesToTake)\n",
    "words_counts = vector.fit_transform(hamTrainDir)\n",
    "words_counts = words_counts.toarray()\n",
    "feature_ham = vector.get_feature_names()\n",
    "\n",
    "#get spam top features\n",
    "vector2 = TfidfVectorizer(stop_words = stop_words_given,\n",
    "                          analyzer='word',\n",
    "                          decode_error = 'ignore',\n",
    "                          max_features = amountOfFeaturesToTake)\n",
    "\n",
    "words_counts2 = vector2.fit_transform(spamTrainDir)\n",
    "words_counts2 = words_counts2.toarray()\n",
    "feature_spam = vector2.get_feature_names()\n",
    "\n",
    "#top features\n",
    "feature_email = feature_ham + feature_spam\n",
    "feature_email = set(feature_email)\n",
    "feature_email = list(feature_email)\n",
    "\n",
    "#Term Document Matrix\n",
    "vector3 = TfidfVectorizer(decode_error = 'ignore',\n",
    "                          vocabulary = feature_email)\n",
    "words_counts3 = vector3.fit_transform(emailTrainDir)\n",
    "words_counts3 = words_counts3.toarray()\n",
    "#---------------------------------------classification: training and testing-------------------------------------------\n",
    "#training labels\n",
    "y = np.zeros(2*amountOfSamplesPerSet)\n",
    "for index in range(amountOfSamplesPerSet, len(y)):\n",
    "    y[index] = 1\n",
    "#testing labels, zero: ham label, one: spam label\n",
    "y1 = np.zeros(len(hamTestDir))\n",
    "y2 = np.ones(len(spamTestDir))\n",
    "\n",
    "#training: Multinomial Naive Bayes\n",
    "clf = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True).fit(words_counts3, y)\n",
    "#ham test\n",
    "new_counts = vector3.transform(hamTestDir)\n",
    "#prediction and accuracy score\n",
    "predicted = clf.predict(new_counts)\n",
    "accu_score = clf.score(new_counts, y1)\n",
    "print (accu_score)\n",
    "\n",
    "#spam test \n",
    "new_counts = vector3.transform(spamTestDir)\n",
    "\n",
    "#prediction accuracy score\n",
    "predicted = clf.predict(new_counts)\n",
    "accu_score = clf.score(new_counts, y2)\n",
    "print (accu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
